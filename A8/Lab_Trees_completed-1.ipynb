{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Install the package gdown if you haven't already! It's only on pip.\n",
    "# !pip install gdown\n",
    "import gdown\n",
    "\n",
    "import pydotplus\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plot inline\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some fake data\n",
    "def make_regression_data(n=25):\n",
    "    x = np.random.uniform(size = (n,1))\n",
    "    e = np.random.normal(0, 0.3, size = x.shape)\n",
    "    y = np.sin(2*np.pi*x) + e\n",
    "    return (x,y.ravel())\n",
    "\n",
    "x,y = make_regression_data()\n",
    "fig, ax = plt.subplots(dpi = 120)\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a regression tree to this data\n",
    "# With varying max depth\n",
    "# To see how this tree grows\n",
    "\n",
    "depths = np.arange(1,7)\n",
    "fig, ax = plt.subplots(dpi = 120, nrows = 2, ncols = 3, figsize = (10,6))\n",
    "plt.subplots_adjust(hspace=0.25)\n",
    "ax = ax.ravel() #flattens the ax variable\n",
    "\n",
    "newx = np.linspace(0,1,101).reshape(-1,1)\n",
    "\n",
    "for d,a in zip(depths, ax):\n",
    "    reg = DecisionTreeRegressor(max_depth=d)\n",
    "    reg.fit(x,y)\n",
    "    \n",
    "    ypred = reg.predict(newx)\n",
    "    a.scatter(x,y)\n",
    "    a.plot(newx, ypred, color = 'red')\n",
    "    a.set_title(f'max_depth = {d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DOT data\n",
    "dot_data = export_graphviz(reg, out_file=None, rounded = True, max_depth=3, feature_names=['x'])\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare variance of predictions between a tree and a bagged estimator\n",
    "\n",
    "# Create a tree model and a bagging from these trees\n",
    "newx = np.linspace(0,1,101).reshape(-1,1)\n",
    "tree = DecisionTreeRegressor(max_depth = 5)\n",
    "bag1 = BaggingRegressor(DecisionTreeRegressor(max_depth = 5), \n",
    "                        n_estimators=100,\n",
    "                       n_jobs = -1)\n",
    "\n",
    "models = [tree, bag1,]\n",
    "model_names = ['Tree', \"Bagged\"]\n",
    "\n",
    "# Create two plots, one for the tree, one for the bagging\n",
    "fig, ax = plt.subplots(dpi = 120, nrows = 1, ncols = len(models), figsize = (10,3), sharey = True)\n",
    "ax = ax.ravel() #flattens the ax variable\n",
    "\n",
    "# Generate 500 runs for each model and calculate bias and variance. This takes a while!\n",
    "nsim = 500\n",
    "\n",
    "for axis, model, name in zip(ax, models, model_names):\n",
    "    \n",
    "    #Store the predictions somewhere\n",
    "    predictions = np.zeros((nsim, newx.shape[0]))\n",
    "    \n",
    "    for i in range(nsim):\n",
    "        x,y = make_regression_data()\n",
    "        model.fit(x,y)\n",
    "        ypred = model.predict(newx)\n",
    "        predictions[i] = ypred\n",
    "        \n",
    "    #Plot the variance\n",
    "    axis.plot(newx, predictions.var(axis = 0), label = 'Variance')\n",
    "    \n",
    "    #Plot the bias\n",
    "    bias = (predictions - np.sin(2*np.pi*newx.T)).mean(axis = 0)**2\n",
    "    axis.plot(newx, bias, color = 'red', label = 'Squared Bias')\n",
    "    axis.set_title(name)\n",
    "    axis.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Now we will train a random forest. It is included in the ```sklearn.ensemble``` subpackage, function [```RandomForestClassifier```](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), so it is straightforward to use. It comes with many parameters, but in general there is a philosophy to follow:\n",
    "\n",
    "- In a Random Forest we want each tree to be large, and to learn as much as possible from its subset of data. We don't care too much if each tree is overadjusted, as we can always increase the number of trees to take care of this.\n",
    "\n",
    "- This said, a good idea is to limit the minimum number of samples per leaf when we have few cases (this is not usually a problem in large trees.)\n",
    "\n",
    "- We might want to limit the minimum impurity decrease to stop growing a tree if not much is happening.\n",
    "\n",
    "- There is also a class weight to include. It does include one automatically if we use the option ```balanced```.\n",
    "\n",
    "Let's train one and check the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the classifier\n",
    "bankloan_rf = RandomForestClassifier(n_estimators=1000, # Number of trees to train\n",
    "                       criterion='gini', # How to train the trees. Also supports entropy.\n",
    "                       max_depth=None, # Max depth of the trees. Not necessary to change.\n",
    "                       min_samples_split=2, # Minimum samples to create a split.\n",
    "                       min_samples_leaf=0.001, # Minimum samples in a leaf. Accepts fractions for %. This is 0.1% of sample.\n",
    "                       min_weight_fraction_leaf=0.0, # Same as above, but uses the class weights.\n",
    "                       max_features='auto', # Maximum number of features per split (not tree!) by default is sqrt(vars)\n",
    "                       max_leaf_nodes=None, # Maximum number of nodes.\n",
    "                       min_impurity_decrease=0.0001, # Minimum impurity decrease. This is 10^-3.\n",
    "                       bootstrap=True, # If sample with repetition. For large samples (>100.000) set to false.\n",
    "                       oob_score=True,  # If report accuracy with non-selected cases.\n",
    "                       n_jobs=-1, # Parallel processing. Set to -1 for all cores. Watch your RAM!!\n",
    "                       random_state=20190305, # Seed\n",
    "                       verbose=1, # If to give info during training. Set to 0 for silent training.\n",
    "                       warm_start=False, # If train over previously trained tree.\n",
    "                       class_weight='balanced'\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train. I have created a credit risk dataset that can be used to predict the probability of not paying back a loan (a credit score). We will also split the data into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset from a shared Google Drive link I have already prepared.\n",
    "url = 'https://drive.google.com/uc?id=1-RiFAF4zU27N9MnoSYUlNuqFhR3VcuWs'\n",
    "output = 'BankloanClean.pkl'\n",
    "gdown.download(url, output, quiet = False)\n",
    "\n",
    "# Read from the file we just got.\n",
    "bankloan_data = pd.read_pickle('BankloanClean.pkl')\n",
    "\n",
    "# Drop a categorical variable\n",
    "bankloan_data.drop('Education', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has the following variables:\n",
    "\n",
    "- Customer: ID, or unique label, of the borrower (NOT predictive).\n",
    "-    Age: Age of the borrower in years.\n",
    "-    Employ: Years at current job.\n",
    "-    Address: Years at current address.\n",
    "-    Income: Income in ‘000s USD.\n",
    "-    Leverage: Debt/Income Ratio.\n",
    "-    CredDebt: Credit card standing debt.\n",
    "-    OthDebt: Other debt in ‘000s USD.\n",
    "-    MonthlyLoad: Monthly percentage from salary used to repay debts.\n",
    "-    Default: 1 If default has occurred, 0 if not (Target variable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankloan_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test, fixing seed.\n",
    "bankloan_train_noWoE, bankloan_test_noWoE = train_test_split(bankloan_data.iloc[:, 1:], # Data \n",
    "                                                             test_size = 0.3,           # Size of test\n",
    "                                                             random_state = 20201107)   # Seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the RF.\n",
    "bankloan_rf.fit(bankloan_train_noWoE.iloc[:,:-1], \n",
    "               bankloan_train_noWoE['Default'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see it used two jobs (two processors are available to me in this computer). It converges very quickly. Let's check how it did, this time we will print a nicer confusion matrix using seaborn, and will plot the ROC curve of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the test set.\n",
    "rf_pred_class_test = bankloan_rf.predict(bankloan_test_noWoE.iloc[:, :-1])\n",
    "rf_probs_test = bankloan_rf.predict_proba(bankloan_test_noWoE.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "confusion_matrix_rf = confusion_matrix(y_true = bankloan_test_noWoE['Default'], \n",
    "                    y_pred = rf_pred_class_test)\n",
    "\n",
    "# Turn matrix to percentages\n",
    "confusion_matrix_rf = confusion_matrix_rf.astype('float') / confusion_matrix_rf.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Turn to dataframe\n",
    "df_cm = pd.DataFrame(\n",
    "        confusion_matrix_rf, index=['good', 'bad'], columns=['good', 'bad'], \n",
    ")\n",
    "\n",
    "# Parameters of the image\n",
    "figsize = (10,7)\n",
    "fontsize=14\n",
    "\n",
    "# Create image\n",
    "fig = plt.figure(figsize=figsize)\n",
    "heatmap = sns.heatmap(df_cm, annot=True, fmt='.2f')\n",
    "\n",
    "# Make it nicer\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, \n",
    "                             ha='right', fontsize=fontsize)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45,\n",
    "                             ha='right', fontsize=fontsize)\n",
    "\n",
    "# Add labels\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Plot!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks a bit unbalanced, but otherwise ok. It's harder to predict the  defaulters. Now let's see the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ROC curve points\n",
    "fpr, tpr, thresholds = roc_curve(bankloan_test_noWoE['Default'], rf_probs_test[:,1])\n",
    "\n",
    "# Save the AUC in a variable to display it. Round it first\n",
    "auc = np.round(roc_auc_score(y_true = bankloan_test_noWoE['Default'], \n",
    "                             y_score = rf_probs_test[:,1]),\n",
    "              decimals = 3)\n",
    "\n",
    "# Create and show the plot\n",
    "plt.plot(fpr,tpr,label=\"Bankloan RF, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's print the variable importance. The importance is calculated by averaging the accuracy of trees when the variables is included the tree, and comparing it to when it's NOT included the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variable importance\n",
    "importances = bankloan_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] \n",
    "\n",
    "f, ax = plt.subplots(figsize=(3, 8))\n",
    "plt.title(\"Variable Importance - Random Forest\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(y=[bankloan_train_noWoE.iloc[:, :-1].columns[i] for i in indices], x=importances[indices], \n",
    "            label=\"Total\", color=\"b\")\n",
    "ax.set(ylabel=\"Variable\",\n",
    "       xlabel=\"Variable Importance (Gini)\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoosting\n",
    "\n",
    "The stochastic gradient boosting model is the alternative to Random Forest. Now we want to create a series of small trees, which will be poorer in performance, but together they will be stronger. Training an XGBoost model is harder, because we need to control the model so it creates small trees, but it performs better in small data, something Random Forests do not necessarily accomplish.\n",
    "\n",
    "While scikit-learn does have its own implementation of XGB ([```sklearn.ensemble```](https://scikit-learn.org/stable/modules/ensemble.html)), there are a couple of very strong packages out there that implement the algorithm. ```xgboost``` and ```lightgbm``` are two of the best known ones. We will use [```xgboost```](https://xgboost.readthedocs.io/en/latest/python/) for this lab, available pretty much for every language out there.\n",
    "\n",
    "The first step is to define a classifier that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the classifier.\n",
    "XGB_Bankloan = XGBClassifier(max_depth=3,                 # Depth of each tree\n",
    "                            learning_rate=0.1,            # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=100,             # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            objective='binary:logistic',  # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=2,                     # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=1,                  # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
    "                            reg_lambda=0,                 # Regularizer for first fit.\n",
    "                            scale_pos_weight=1,           # Balancing of positive and negative weights.\n",
    "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
    "                            random_state=20201107,        # Seed\n",
    "                            missing=None                  # How are nulls encoded?\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier can be used to tune the parameters of the model. We will use sklearn's ```GridSearchCV``` for this. It requires a dictionary of the parameters to look for. We will tune the number of trees (XGB overfits relatively easily, always tune this), the depth, and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters. Play with this grid!\n",
    "param_grid = dict({'n_estimators': [50, 100, 150],\n",
    "                   'max_depth': [2, 3],\n",
    "                 'learning_rate' : [0.001, 0.01, 0.1]\n",
    "                  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a validation set for the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always a good idea to tune on a reduce sample of the train set, as we will call many functions.\n",
    "val_train = bankloan_train_noWoE.sample(frac = 0.3,               # The fraction to extract\n",
    "                                       random_state = 20201107    # The seed.\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a grid search over the parameter space. We will use the AUC (as this is a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-22c1e64a350a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define grid search object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m GridXGB = GridSearchCV(XGB_Bankloan,        # Original XGB. \n\u001b[0m\u001b[1;32m      3\u001b[0m                        \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# Parameter grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m              \u001b[0;31m# Number of cross-validation folds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#                        scoring = 'roc_auc', # How to rank outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "# Define grid search object.\n",
    "GridXGB = GridSearchCV(XGB_Bankloan,        # Original XGB. \n",
    "                       param_grid,          # Parameter grid\n",
    "                       cv = 3,              # Number of cross-validation folds.  \n",
    "                    scoring = 'roc_auc', # How to rank outputs.\n",
    "                       n_jobs = -1,         # Parallel jobs. -1 is \"all you have\"\n",
    "                       refit = False,       # If refit at the end with the best. We'll do it manually.\n",
    "                       verbose = 1          # If to show what it is doing.\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train grid search.\n",
    "GridXGB.fit(val_train.iloc[:, :-1], val_train['Default'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can output the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best params\n",
    "GridXGB.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is telling us to use 10% learning rate with a max_depth of 3 and 100 trees. As the max_depth parameter is at the limit, I would run again with a depth of four just to check one further the limit. I leave this as an exercise.\n",
    "\n",
    "Now we can fit the final model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGB with best parameters.\n",
    "XGB_Bankloan = XGBClassifier(max_depth=GridXGB.best_params_.get('max_depth'), # Depth of each tree\n",
    "                            learning_rate=GridXGB.best_params_.get('learning_rate'), # How much to shrink error in each subsequent training. Trade-off with no. estimators.\n",
    "                            n_estimators=GridXGB.best_params_.get('n_estimators'), # How many trees to use, the more the better, but decrease learning rate if many used.\n",
    "                            verbosity=1,                  # If to show more errors or not.\n",
    "                            objective='binary:logistic',  # Type of target variable.\n",
    "                            booster='gbtree',             # What to boost. Trees in this case.\n",
    "                            n_jobs=2,                     # Parallel jobs to run. Set your processor number.\n",
    "                            gamma=0.001,                  # Minimum loss reduction required to make a further partition on a leaf node of the tree. (Controls growth!)\n",
    "                            subsample=1,                  # Subsample ratio. Can set lower\n",
    "                            colsample_bytree=1,           # Subsample ratio of columns when constructing each tree.\n",
    "                            colsample_bylevel=1,          # Subsample ratio of columns when constructing each level. 0.33 is similar to random forest.\n",
    "                            colsample_bynode=1,           # Subsample ratio of columns when constructing each split.\n",
    "                            reg_alpha=1,                  # Regularizer for first fit. alpha = 1, lambda = 0 is LASSO.\n",
    "                            reg_lambda=0,                 # Regularizer for first fit.\n",
    "                            scale_pos_weight=1,           # Balancing of positive and negative weights.\n",
    "                            base_score=0.5,               # Global bias. Set to average of the target rate.\n",
    "                            random_state=20201107,        # Seed\n",
    "                            missing=None                  # How are nulls encoded?\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train over all training data.\n",
    "XGB_Bankloan.fit(bankloan_train_noWoE.iloc[:, :-1], bankloan_train_noWoE['Default'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our model. First we calculate the variable importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variable importance\n",
    "importances = XGB_Bankloan.feature_importances_\n",
    "indices = np.argsort(importances)[::-1] \n",
    "\n",
    "f, ax = plt.subplots(figsize=(3, 8))\n",
    "plt.title(\"Variable Importance - XGBoosting\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(y=[bankloan_train_noWoE.iloc[:, :-1].columns[i] for i in indices], x=importances[indices], \n",
    "            label=\"Total\", color=\"b\")\n",
    "ax.set(ylabel=\"Variable\",\n",
    "       xlabel=\"Variable Importance (Gini)\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see here? Does it make sense to you?\n",
    "\n",
    "Let's finish by plotting the ROC curve. How does it compare to Random Forest? Why do you think this is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probability\n",
    "probTest = XGB_Bankloan.predict_proba(bankloan_test_noWoE.iloc[:, :-1])\n",
    "probTest = probTest[:, 1]\n",
    "\n",
    "# Calculate the ROC curve points\n",
    "fpr, tpr, thresholds = roc_curve(bankloan_test_noWoE['Default'], \n",
    "                                 probTest)\n",
    "\n",
    "# Save the AUC in a variable to display it. Round it first\n",
    "auc = np.round(roc_auc_score(y_true = bankloan_test_noWoE['Default'], \n",
    "                             y_score = probTest),\n",
    "               decimals = 3)\n",
    "\n",
    "# Create and show the plot\n",
    "plt.plot(fpr,tpr,label=\"AUC - XGBoosting = \" + str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
